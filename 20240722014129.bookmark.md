# An Introduction to Open Domain Question-Answering | Pinecone (www.pinecone.io)

<https://www.pinecone.io/learn/series/nlp/question-answering/>

## Content

[](https://docs.pinecone.io/guides/indexes/understanding-indexes#cloud-regions){.none}

AnnouncementPinecone serverless is now in public preview on GCP and AzureGet started![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMCIgaGVpZ2h0PSIxMCIgdmlld2JveD0iOC41OSAzLjU5IDEyLjI0IDE4LjgzIiBjbGFzcz0idHJhbnNsYXRlLXktWzFweF0gZmlsbC1hbHBoYTIiPjxwYXRoIGQ9Ik0gMTEuNDE0MDYzIDMuNTg1OTM4IEwgOC41ODU5MzggNi40MTQwNjMgTCAxNS4xNzE4NzUgMTMgTCA4LjU4NTkzOCAxOS41ODU5MzggTCAxMS40MTQwNjMgMjIuNDE0MDYzIEwgMjAuODI4MTI1IDEzIFoiIC8+PC9zdmc+){.translate-y-[1px] .fill-alpha2}

[![](data:image/svg+xml;base64,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){.w-full .-translate-y-[4px]}](/){.w-32 .shrink-0 .focus:outline-offset-4 .focus:outline-alpha2 aria-label="Pinecone"}

[Product](/product/){.flex .items-center .border-y-2 .border-transparent .text-base .transition-colors .duration-300 .hover:border-b-alpha2 .hover:text-alpha2 .focus:border-b-alpha2 .focus:text-alpha2 .focus:outline-none}[Pricing](/pricing/){.flex .items-center .border-y-2 .border-transparent .text-base .transition-colors .duration-300 .hover:border-b-alpha2 .hover:text-alpha2 .focus:border-b-alpha2 .focus:text-alpha2 .focus:outline-none}[Learn](/learn/){.flex .items-center .border-y-2 .border-transparent .text-base .transition-colors .duration-300 .hover:border-b-alpha2 .hover:text-alpha2 .focus:border-b-alpha2 .focus:text-alpha2 .focus:outline-none}[Company](/company/){.flex .items-center .border-y-2 .border-transparent .text-base .transition-colors .duration-300 .hover:border-b-alpha2 .hover:text-alpha2 .focus:border-b-alpha2 .focus:text-alpha2 .focus:outline-none}[Docs](https://docs.pinecone.io){.flex .items-center .border-y-2 .border-transparent .text-base .transition-colors .duration-300 .hover:border-b-alpha2 .hover:text-alpha2 .focus:border-b-alpha2 .focus:text-alpha2 .focus:outline-none}

![](data:image/svg+xml;base64,PHN2ZyBpZD0iaWNvbiIgY2xhc3M9InotMTAgZ3JvdXAtaG92ZXI6YnJpZ2h0bmVzcy01MCIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE3IiB2aWV3Ym94PSIwIDAgMTYgMTciIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzIyMDhfODA4NikiPjxwYXRoIGQ9Ik02LjUgMTEuNzI1MUM5LjI2MTQyIDExLjcyNTEgMTEuNSA5LjQ4NjUyIDExLjUgNi43MjUxQzExLjUgMy45NjM2NyA5LjI2MTQyIDEuNzI1MSA2LjUgMS43MjUxQzMuNzM4NTggMS43MjUxIDEuNSAzLjk2MzY3IDEuNSA2LjcyNTFDMS41IDkuNDg2NTIgMy43Mzg1OCAxMS43MjUxIDYuNSAxMS43MjUxWiIgc3Ryb2tlPSIjNTI1MjVCIiBzdHJva2Utd2lkdGg9IjIiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgLz48cGF0aCBkPSJNMTQuNSAxNC43MjVMMTAuMDM1IDEwLjI2IiBzdHJva2U9IiM1MjUyNUIiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiAvPjwvZz48ZGVmcz48Y2xpcHBhdGggaWQ9ImNsaXAwXzIyMDhfODA4NiI+PHJlY3Qgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2IiBmaWxsPSJ3aGl0ZSIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMCAwLjUpIiAvPjwvY2xpcHBhdGg+PC9kZWZzPjwvc3ZnPg==){#icon .z-10 .group-hover:brightness-50}

[Login](https://app.pinecone.io/?sessionType=login){.hidden .p-1 .text-[0.9375rem]/[150%] .text-zinc-600 .hover:text-alpha2 .focus:text-alpha2 .focus:outline-alpha2 .lg:block}[Sign up](https://app.pinecone.io/?sessionType=signup){.hidden .rounded-full .bg-alpha2 .px-5 .py-2.5 .text-center .text-[0.9375rem]/[140%] .text-white .transition-colors .duration-300 .hover:bg-alpha1 .focus:outline-offset-2 .focus:outline-alpha1 .lg:block state="fixed"}

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTIiIHZpZXdib3g9IjAgMCAxNCAxMiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJNMCAwSDE0VjEuNUgwVjBaTTAgNUgxNFY2LjVIMFY1Wk0xNCAxMFYxMS41SDBWMTBIMTRaIiBmaWxsPSJibGFjayIgLz48L3N2Zz4=)

# An Introduction to Open Domain Question-Answering {#an-introduction-to-open-domain-question-answering .text-h1-mobile .xl:text-h1 .text-black .font-normal .xl:mr-[20px] .leading-[48.40px] .text-[40px] .text-left .xl:min-w-[630px] .xl:max-w-[630px]\"}

Jump to section ![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTEiIGhlaWdodD0iMTIiIHZpZXdib3g9IjAgMCAxMiAxMyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiBjbGFzcz0ibXQtMC41Ij48cmVjdCB4PSIxMiIgeT0iNS41IiB3aWR0aD0iMiIgaGVpZ2h0PSIxMiIgdHJhbnNmb3JtPSJyb3RhdGUoOTAgMTIgNS41KSIgZmlsbD0iI0I3QjdERCIgLz48L3N2Zz4=){.mt-0.5}

-   [Question Answering at a Glance](#Question-Answering-at-a-Glance){.hover:underline .hover:opacity-50 .cursor-pointer}
-   [Further Reading](#Further-Reading){.hover:underline .hover:opacity-50 .cursor-pointer}

------------------------------------------------------------------------

Search is a crucial functionality in many applications and companies globally. Whether in manufacturing, finance, healthcare, or *almost* any other industry, organizations have vast internal information and document repositories.

Unfortunately, the scale of many companies' data means that the organization and accessibility of information can become incredibly inefficient. The problem is exacerbated for language-based information. Language is a tool for people to communicate often abstract ideas and concepts. Naturally, ideas and concepts are harder for a computer to comprehend and store in a meaningful way.

Most organizations rely on a cluster of keyword-based search interfaces hosted on various *'internal portals'* to deal with language data. If done well, this can satisfy business requirements for *some* of that data.

If a person knows what they're looking for and they know the keywords and terminology of the information they need, a keyword-based search is ideal. When the keywords and terminology of the answer are unknown, keyword search is inadequate. People searching for unknown answers in large repositories of documents is a drain on productivity.

How do we minimize this problem? The answer lies with *semantic search*, specifically with the question-answering (QA) flavor of semantic search.

Semantic search allows us to [search based on concepts and ideas](/learn/series/nlp/sentence-embeddings/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline} rather than keywords. Given a phrase, a semantic search tool returns the most *semantically similar* phrases from a repository.

Question-answering takes this idea further by searching using a natural language question and returning relevant documents and specific answers. QA aims to mimic natural language as much as possible. If we asked a shop assistant *"where are those tasty, freshly baked things that are not cookies but look like cookies?\"*, we would expect directions that take us to those things. This natural form of conversation is what QA aims to reproduce.

This article will introduce the different forms of QA, the components of these *'QA stacks'*, and where we might use them.

## Question Answering at a Glance {#Question-Answering-at-a-Glance .mt-50 .text-h2-mobile .lg:text-h2 .text-black}

Before we dive into the details, let us paint a high-level picture of QA. First, our focus is on *open-domain* QA (ODQA). ODQA systems deal with questions across broad topics and cannot rely on a specific set of rules in your code. The alternative to *open-domain* is *closed-domain*, which focuses on a limited domain/scope and *can* often rely on explicit logic. We will **not** cover *closed-domain* QA.

For the remainder of the article, I will use **OD**QA and QA interchangeably. ODQA models can be split into a few subcategories.

![There are a few approaches to question answering (QA).](/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fbaf3d22b6f9639858614098473625063abf2853a-1920x890.png&w=3840&q=75){.max-w-full .h-auto .hover:cursor-zoom-in loading="lazy" width="1920" height="890" decoding="async" nimg="1" style="color:transparent" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fbaf3d22b6f9639858614098473625063abf2853a-1920x890.png&w=1920&q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fbaf3d22b6f9639858614098473625063abf2853a-1920x890.png&w=3840&q=75 2x"}

There are a few approaches to question answering (QA).

The most common form of QA is **open-book extractive QA** (top-left above). Here we combine an information retrieval (IR) step and a reading comprehension (RC) step.

Any *open-book* QA requires an IR step to *retrieve* relevant information from the 'open-book'. Just as with open-book exams where students can refer to their books for information during the exam, the model can refer to an external source of information. That source of information may be internal company documents, Wikipedia, Reddit, or any other information source that *is not* the model itself.

The IR step retrieves relevant documents and passes them to the RC (reader) step. RC consists of *extracting* a succinct answer from a sentence or paragraph, typically referred to as the *document* or *context*.

![Example of a question, relevant context, and an answer.](/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7b650592a1d9a902d7e8682bcda52617411d10a4-1920x860.png&w=3840&q=75){.max-w-full .h-auto .hover:cursor-zoom-in loading="lazy" width="1920" height="860" decoding="async" nimg="1" style="color:transparent" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7b650592a1d9a902d7e8682bcda52617411d10a4-1920x860.png&w=1920&q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7b650592a1d9a902d7e8682bcda52617411d10a4-1920x860.png&w=3840&q=75 2x"}

Example of a question, relevant context, and an answer.

The other two types of QA rely on *generating* answers rather than *extracting* them. OpenAI's GPT models are well-known generative transformer models.

In *open-book* abstractive QA, the first IR step is the same as extractive QA; relevant contexts are *retrieved* from an external source. These contexts are passed to the text generation model (such as GPT) and used to *generate* (not extract) an answer.

Alternatively, we can use *closed-book* abstractive QA. Here there is only a text generation model and *no* IR step. The generator model will generate an answer based on its own internal learned representation of the world. It *cannot* refer to any external source of information hence the name *closed-book*.

Let's dive into each of these approaches and learn where we might apply each.

### Extractive QA {#Extractive-QA .mt-50 .text-h3-mobile .lg:text-h3 .text-black}

Extractive QA is arguably the most widely applicable form of question-answering. It allows us to ask a question and then *extract* an answer from a short text. For example, we have the text (or *context*):

Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24--10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\'s Stadium in the San Francisco Bay Area at Santa Clara, California.

To which we could ask the question, \"which team represented the AFC at Super Bowl 50?\" and we should expect to return \"Denver Broncos\".

The example where we present a single *context* and extract an answer is reading comprehension (RC). Alone, this is not particularly useful, but we can couple it with an external data source and search through *many contexts*, not just one. We call this 'open-book extractive QA'. More commonly referred to as just *extractive QA*. It is not a single model but actually consists of *three* components:

-   Indexed data (document store/vector database)
-   Retriever model
-   Reader model

Before beginning to ask questions, open-book QA requires indexing data that our retriever model can later access. Typically this will be chunks of sentence-to-paragraph-sized text.

![The open-book extractive QA stack includes the 'open-book' database, a retriever model, and a reader model.](/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fb10abf5c6ae4b5d88564527c19c827c5df862c82-1920x840.png&w=3840&q=75){.max-w-full .h-auto .hover:cursor-zoom-in loading="lazy" width="1920" height="840" decoding="async" nimg="1" style="color:transparent" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fb10abf5c6ae4b5d88564527c19c827c5df862c82-1920x840.png&w=1920&q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fb10abf5c6ae4b5d88564527c19c827c5df862c82-1920x840.png&w=3840&q=75 2x"}

The open-book extractive QA stack includes the 'open-book' database, a retriever model, and a reader model.

Let's work through an example. First, we need data. A popular QA dataset is the Stanford Question and Answering Dataset (SQuAD). We can download this dataset using Hugging Face's datasets library like so:

In\[1\]:

``` {.relative .language-python .overflow-auto}
import datasets

qa = datasets.load_dataset('squad', split='validation')
qa
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[1\]:

``` {.overflow-auto .pb-25 .pr-25}
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers'],
    num_rows: 10570
})
```

In\[2\]:

``` {.relative .language-python .overflow-auto}
qa[0]
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[2\]:

``` {.overflow-auto .pb-25 .pr-25}
{'id': '56be4db0acb8001400a502ec',
 'title': 'Super_Bowl_50',
 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the "golden anniversary" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as "Super Bowl L"), so that the logo could prominently feature the Arabic numerals 50.',
 'question': 'Which NFL team represented the AFC at Super Bowl 50?',
 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],
  'answer_start': [177, 177, 177]}}
```

Here we have the *context* feature. It is these contexts that should be indexed in our database.

Options for the type of database vary based on the retriever model. A traditional retriever uses *sparse vector* retrieval with TF-IDF or BM25. These models return *contexts* based on the frequency of matching words between a *context* and the *question*. More word matches equate to higher relevance. Elasticsearch is the most popular database solution for this thanks to their scalable and strong keyword search capabilities.

The other option is to use *dense vector* retrieval with sentence vectors built by [transformer models](/learn/transformers/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline} like [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}. Dense vectors have the advantage of enabling search via *semantics*. Searching with the meaning of a question as described in the *'tasty, freshly baked things'* example. For this, a vector database like [Pinecone](/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline} or a standalone vector index like [Faiss](/learn/series/faiss/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline} is needed.

We will try the *dense vector* approach. First, we encode our *contexts* with a QA model like multi-qa-MiniLM-L6-cos-v1 from sentence-transformers. We initialize the model with:

In\[5\]:

``` {.relative .language-python .overflow-auto}
!pip install sentence-transformers
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')
model
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[5\]:

``` {.overflow-auto .pb-25 .pr-25}
SentenceTransformer(
  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
  (2): Normalize()
)
```

Using the model, we encode the contexts inside our dataset object qa to create the sentence vector representations to be indexed in our vector database.

In\[6\]:

``` {.relative .language-python .overflow-auto}
qa = qa.map(lambda x: {
    'encoding': model.encode(x['context']).tolist()
}, batched=True, batch_size=32)
qa
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[6\]:

``` {.overflow-auto .pb-25 .pr-25}
100%|██████████| 65/65 [08:57<00:00,  8.26s/ba]
```

Out\[6\]:

``` {.overflow-auto .pb-25 .pr-25}
Dataset({
    features: ['answers', 'context', 'encoding', 'id', 'question', 'title'],
    num_rows: 2067
})
```

Now we can go ahead and store these inside a vector database. We will use Pinecone in this example (which does require a [free API key](https://app.pinecone.io/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}). First, we initialize a connection to Pinecone, create a new index, and connect to it.

In\[8\]:

``` {.relative .language-python .overflow-auto}
!pip install pinecone-client
import pinecone

pinecone.init(
    api_key=API_KEY,
    environment='YOUR_ENV'  # find next to API key in console
)

# check if index already exists, if not we create it
if 'qa-index' not in pinecone.list_indexes():
    pinecone.create_index(
        name='qa-index',
        dimension=len(qa[0]['encoding'])
    )

# connect to index
index = pinecone.Index('qa-index')
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

From there, all we need to do is upsert (*up*load and in*sert*) our vectors to the Pinecone index. We do this in batches where each sample is a tuple of (id, vector).

In\[9\]:

``` {.relative .language-python .overflow-auto}
from tqdm.auto import tqdm  # progress bar

upserts = [(v['id'], v['encoding']) for v in qa]
# now upsert in chunks
for i in tqdm(range(0, len(upserts), 50)):
    i_end = i + 50
    if i_end > len(upserts): i_end = len(upserts)
    index.upsert(vectors=upserts[i:i_end])
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[9\]:

``` {.overflow-auto .pb-25 .pr-25}
100%|██████████| 42/42 [00:27<00:00,  1.51it/s]
```

Once the contexts have been indexed inside the database, we can move on to the QA process.

Given a question/query, the *retriever* creates a sparse/dense vector representation called a *query vector*. This query vector is compared against all of the already indexed *context vectors* in the database. The *n* most similar are returned.

In\[10\]:

``` {.relative .language-python .overflow-auto}
query = "Which NFL team represented the AFC at Super Bowl 50?"
xq = model.encode([query]).tolist()
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

In\[11\]:

``` {.relative .language-python .overflow-auto}
xc = index.query(xq, top_k=5)
xc
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[11\]:

``` {.overflow-auto .pb-25 .pr-25}
{'results': [{'matches': [{'id': '56be4db0acb8001400a502ec',
                           'score': 0.685847521,
                           'values': []},
                             ...
                          {'id': '56bec0dd3aeaaa14008c9357',
                           'score': 0.520058692,
                           'values': []}],
              'namespace': ''}]}
```

In\[12\]:

``` {.relative .language-python .overflow-auto}
ids = [x['id'] for x in xc['results'][0]['matches']]
contexts = qa.filter(lambda x: True if x['id'] in ids else False)
contexts['context']
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[12\]:

``` {.overflow-auto .pb-25 .pr-25}
100%|██████████| 3/3 [00:00<00:00,  7.84ba/s]
```

Out\[12\]:

``` {.overflow-auto .pb-25 .pr-25}
['Super Bowl 50 was an American football game...',
 'The Panthers finished the regular season with...',
 'In early 2012, NFL Commissioner Roger Goodell...',
 'In the United States, the game was televised...',
 "Super Bowl 50 featured numerous records from..."]
```

These most similar contexts are passed (one at a time) to the *reader* model alongside the original question. Given a question and context, the reader predicts the start and end positions of an answer.

![The reader model predicts the start and end positions of an answer given a question and a context containing the answer.](/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F80e472c65a1785a9b84aaa8456f7940d69677fca-1920x1005.png&w=3840&q=75){.max-w-full .h-auto .hover:cursor-zoom-in loading="lazy" width="1920" height="1005" decoding="async" nimg="1" style="color:transparent" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F80e472c65a1785a9b84aaa8456f7940d69677fca-1920x1005.png&w=1920&q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F80e472c65a1785a9b84aaa8456f7940d69677fca-1920x1005.png&w=3840&q=75 2x"}

The reader model predicts the start and end positions of an answer given a question and a context containing the answer.

We will use the deepest/electra-base-squad2 model from HuggingFace's transformers as our reader model. All we do is set up a \'question-answering\' pipeline and pass our *query* and *contexts* to it one by one.

In\[14\]:

``` {.relative .language-python .overflow-auto}
from transformers import pipeline

model_name = 'deepset/electra-base-squad2'
nlp = pipeline(tokenizer=model_name, model=model_name, task='question-answering')
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

In\[15\]:

``` {.relative .language-python .overflow-auto}
print(query)
for context in contexts['context']:
    print(nlp(question=query, context=context))
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[15\]:

``` {.overflow-auto .pb-25 .pr-25}
Which NFL team represented the AFC at Super Bowl 50?
{'score': 0.9998526573181152, 'start': 177, 'end': 191, 'answer': 'Denver Broncos'}
{'score': 6.595961963284935e-07, 'start': 525, 'end': 539, 'answer': 'Dallas Cowboys'}
{'score': 1.11751314761932e-05, 'start': 15, 'end': 93, 'answer': 'NFL Commissioner Roger Goodell stated that the league planned to make the 50th'}
{'score': 2.344028617040639e-12, 'start': 564, 'end': 579, 'answer': 'Super Bowl XXXV'}
{'score': 0.009671280160546303, 'start': 68, 'end': 74, 'answer': 'Denver'}
```

The reader prediction is repeated for each context. From here --- if preferred --- we can order the 'answers' using the scores output by the retriever and/or reader models.

As we can see, the model returns the correct answer of \'Denver Broncos\' with a score of 0.99. Most other answers return only minuscule scores, showing that our reader model easily distinguishes between good and bad answers.

### Abstractive QA {#Abstractive-QA .mt-50 .text-h3-mobile .lg:text-h3 .text-black}

As we saw before, abstractive QA can be split into two types: open-book and *closed*-book. We will start with *open-book* as the natural continuation of the previous extractive QA pipeline.

#### Open Book {#Open-Book .text-h4-mobile .lg:text-h4 .text-black style="margin-block:36px"}

Being **open-book** *abstractive* QA, we can use the same database and retriever components used for *extractive QA*. These components work in the same way and deliver a set of *contexts* to our *generator* model, which replaces the *reader* from extractive QA.

![Open-book abstractive QA pipeline, note that the reader model has been replaced with a generator model (highlighted) when compared to the extractive QA stack.](/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5c61c20003ea4a4700ead59529ca394b57f182d2-1920x840.png&w=3840&q=75){.max-w-full .h-auto .hover:cursor-zoom-in loading="lazy" width="1920" height="840" decoding="async" nimg="1" style="color:transparent" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5c61c20003ea4a4700ead59529ca394b57f182d2-1920x840.png&w=1920&q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5c61c20003ea4a4700ead59529ca394b57f182d2-1920x840.png&w=3840&q=75 2x"}

Open-book abstractive QA pipeline, note that the reader model has been replaced with a generator model (highlighted) when compared to the extractive QA stack.

Rather than *extracting* answers, contexts are used as input (alongside the question) to a generative sequence-to-sequence (seq2seq) model. The model uses the question and context to *generate* an answer.

Large transformer models store 'representations' of knowledge in their parameters. By passing relevant contexts and questions into the model, we *hope* that the model will use the context alongside its 'stored knowledge' to answer more *abstract* questions.

The seq2seq model used is commonly BART or T5-based. We will go ahead and initialize a seq2seq pipeline using a BART model fine-tuned for abstractive QA --- yjernite/bart_eli5.

In\[7\]:

``` {.relative .language-python .overflow-auto}
from transformers import pipeline

model_name = 'yjernite/bart_eli5'
seq2seq = pipeline('text2text-generation', model=model_name, tokenizer=model_name)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

The question we asked before is specific. We're looking for a short and concise answer of Denver Broncos. Abstractive QA is not ideal for these types of questions:

In\[8\]:

``` {.relative .language-python .overflow-auto}
for context in contexts['context']:
    answer = seq2seq(
        f"question: {query} context: {context}",
        num_beams=4,
        do_sample=True,
        temperature=1.5,
        max_length=64
    )
    print(answer)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[8\]:

``` {.overflow-auto .pb-25 .pr-25}
[{'generated_text':
    ' The AFC had one of the two remaining wild card spots. So the team that won the divisional playoffs represented the AFC. It was the same as how the Buffalo Bills represented the AFC in Super Bowl 50.'}]

[{'generated_text':
    ' It was a wild card match, so it was decided to swap teams for the Super Bowl 50 game. The New England Patriots won that match so they were automatically guaranteed to be a part of the game. The New York Jets were a wild card selection so they were assigned a spot in the game.'}]

[{'generated_text':
    " It's kind of like the difference between having a dog and a pony. A dog will give a little extra attention to you but that's it. A pony is a bit more of a nuisance but that's not a big deal."}]

[{'generated_text':
    " Each team has a team coach. Which NFL team you're asking about is a different question. I'm not sure why you're assuming that the AFC is represented at Super Bowl 50. It's an entirely different thing. The AFC is represented by the AFC team that won the AFC Championship last year. Which team"}]

[{'generated_text':
    " The AFC won the AFC Championship. That's the correct answer. The AFC Championship game is the final game of the season between the AFC and AFC. The AFC Championship game is the regular season of the playoffs, which are the playoffs for the teams in the AFC. The AFC Championship is the final game of the"}]
```

Instead, the benefit of abstractive QA comes with more 'abstract' questions like \"Do NFL teams only care about playing at the Super Bowl?\" Here, we're almost asking for an opinion. There is unlikely to be an *exact* answer. Let's see what the abstractive QA method thinks about this.

In\[9\]:

``` {.relative .language-python .overflow-auto}
query = "Do NFL teams only care about playing at the Superbowl?"
xq = retriever.encode([query]).tolist()
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

In\[10\]:

``` {.relative .language-python .overflow-auto}
xc = index.query(xq, top_k=5)
ids = [x['id'] for x in xc['results'][0]['matches']]
contexts = qa.filter(lambda x: True if x['id'] in ids else False)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

In\[11\]:

``` {.relative .language-python .overflow-auto}
for context in contexts['context']:
    answer = seq2seq(
        f"question: {query} context: {context}",
        num_beams=4,
        do_sample=True,
        temperature=1.5,
        max_length=64
    )
    print(answer)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[11\]:

``` {.overflow-auto .pb-25 .pr-25}
[{'generated_text':
    ' No, because it is the pinnacle of professional football. You can only get so good at being a pro team, and the Superbowl is a good example of how great they can be. Most of the teams that play in the Superbowl are teams that have had bad seasons, or are rebuilding teams, or'}]

[{'generated_text':
    " I'm no expert, but I would guess that it's about how well the team is prepared. A team needs to be able to score goals. Even if the opponents have a bunch of players that can't be as good as the team, maybe the team can't score as many goals. Maybe there's"}]

[{'generated_text': 
    " It's not just NFL teams, it's all sports teams. They want to play at the Super Bowl. This is also why the Super Bowl is so important for the world Cup. The world cup is held every two years, the NFL is the one that takes place every four years."}]

[{'generated_text':
    " I'm not sure how you can ask a question that makes no sense. They don't have a right to the Super Bowl, it's just a big event that brings thousands of people to a city. They don't care if they lose, they just care if they get a nice, big crowd to cheer"}]

[{'generated_text':
    ' They are paid a lot of money to be in the Superbowl. It makes them feel a lot better about being a part of it. The NFL owners are paid a lot of money to allow them to play in the Superbowl.'}]
```

These answers look *much* better than our 'specific' question. The returned contexts don't include direct information about whether the teams care about being in the Super Bowl. Instead, they contain snippets of concrete NFL/Super Bowl details.

The seq2seq model combines those details and its own internal 'knowledge' to produce some insightful thoughts on the question:

-   *"No, because it is the pinnacle of professional football"* --- points out that teams in the Super Bowl (whether they win or not) already know they're at the top; in a way, they've *'already won'*.
-   *"They don't care if they lose, they just care if they get a nice, big crowd to cheer"* --- players are happy that they get to entertain their fans; that is, the Super Bowl is less important.
-   *"They are paid a lot of money to be in the Superbowl"* --- points out the more obvious 'who *wouldn't* want bucket loads of money?'.

There is plenty of contradiction and opinion, but that is often the case with more abstract questioning, particularly with the question we asked.

Although these results are interesting, they're not perfect. We can tweak parameters such as temperature to increase/decrease randomness in the answers, but abstractive QA can be limited in its coherence.

#### Closed Book {#Closed-Book .text-h4-mobile .lg:text-h4 .text-black style="margin-block:36px"}

The final architecture we will look at is *closed-book* abstractive QA. In reality, this is nothing more than a generative model that takes a question and relies on *nothing more* than its own internal knowledge. There is **no** retrieval step.

![The closed-book architecture is much simpler, there is nothing more than a generator model.](/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc04350faf5e4bfdea86f835ac104b397ba4c7796-1920x1080.png&w=3840&q=75){.max-w-full .h-auto .hover:cursor-zoom-in loading="lazy" width="1920" height="1080" decoding="async" nimg="1" style="color:transparent" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc04350faf5e4bfdea86f835ac104b397ba4c7796-1920x1080.png&w=1920&q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc04350faf5e4bfdea86f835ac104b397ba4c7796-1920x1080.png&w=3840&q=75 2x"}

The closed-book architecture is much simpler, there is nothing more than a generator model.

Although we're dropping the retriever model, that doesn't mean we stick with the same reader model. As we saw before, the yjernite/bart_eli5 model requires input like:

question: \<our question\> context: \<a (hopefully) relevant context\>

Without the context input, the previous model does not perform as well. This is to be expected. The seq2seq model is optimized to produce coherent answers when given both question *and* context. If our input is in a new, unexpected format, performance suffers:

In\[2\]:

``` {.relative .language-python .overflow-auto}
query = "Do NFL teams only care about playing at the Super Bowl?"

seq2seq(
    f"question: {query} context: unknown",
    num_beams=4,
    do_sample=True,
    temperature=1.5,
    max_length=64
)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[2\]:

``` {.overflow-auto .pb-25 .pr-25}
[{'generated_text': ' I have a follow-up question: Do the NFL teams only have one shot at winning the Super Bowl? Do they only have ONE shot at winning the Super Bowl?'}]
```

The model doesn't know the answer and flips the direction of questioning. Unfortunately, this isn't really what we want. However, there are many alternative models we can try. The GPT models from OpenAI are well-known examples of generative transformers and can produce good results.

GPT-3, the most recent GPT from OpenAI, is locked behind an API, but there are open-source alternatives like GPT-Neo from Eleuther AI. Let's try one of the smaller GPT-Neo models.

In\[3\]:

``` {.relative .language-python .overflow-auto}
gen = pipeline('text-generation', model='EleutherAI/gpt-neo-125M', tokenizer='EleutherAI/gpt-neo-125M')

gen(query, max_length=32)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[3\]:

``` {.overflow-auto .pb-25 .pr-25}
[{'generated_text': 'Do NFL teams only care about playing at the Super Bowl?\n\nThe NFL is a great place to play, but it’s not the only place'}]
```

Here we're using the \'text-generation\' pipeline. All we do here is generate text following a question. We do get an interesting answer which is true but doesn't necessarily answer the question. We can try a few more questions.

In\[5\]:

``` {.relative .language-python .overflow-auto}
gen("Where do cats come from?", max_length=32)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[5\]:

``` {.overflow-auto .pb-25 .pr-25}
[{'generated_text': 'Where do cats come from?\n\nCats are a group of animals that are found in the wild. They are the most common species in the wild,'}]
```

In\[6\]:

``` {.relative .language-python .overflow-auto}
gen("Who was the first person on the moon?", max_length=32)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[6\]:

``` {.overflow-auto .pb-25 .pr-25}
[{'generated_text': "Who was the first person on the moon?\n\nThe moon is the most important part of the Earth's atmosphere. It is the most important part of the"}]
```

In\[7\]:

``` {.relative .language-python .overflow-auto}
gen("What is the moon made of?", max_length=32)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[7\]:

``` {.overflow-auto .pb-25 .pr-25}
[{'generated_text': 'What is the moon made of?\n\nThe moon is a kind of material that is made of a material that is made of a material that is made of'}]
```

We can tweak parameters to reduce the likelihood of repetition.

In\[8\]:

``` {.relative .language-python .overflow-auto}
gen("What is the moon made of?", max_length=32, do_sample=True)
```

![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjQuMjUgMi4yNSAxNS41IDE5LjUiIGNsYXNzPSJ3LTQiPjxwYXRoIGNsYXNzPSJmaWxsLWFscGhhMSBvcGFjaXR5LTQwIGdyb3VwLWhvdmVyOmZpbGwtYWxwaGEyIiBkPSJNMTkuNTMgOCAxNCAyLjQ3YS43NS43NSAwIDAgMC0uNTMtLjIySDExQTIuNzUgMi43NSAwIDAgMCA4LjI1IDV2MS4yNUg3QTIuNzUgMi43NSAwIDAgMCA0LjI1IDl2MTBBMi43NSAyLjc1IDAgMCAwIDcgMjEuNzVoN0EyLjc1IDIuNzUgMCAwIDAgMTYuNzUgMTl2LTEuMjVIMTdBMi43NSAyLjc1IDAgMCAwIDE5Ljc1IDE1VjguNWEuNzUuNzUgMCAwIDAtLjIyLS41Wm0tNS4yOC0zLjE5IDIuOTQgMi45NGgtMi45NFY0LjgxWm0xIDE0LjE5QTEuMjUgMS4yNSAwIDAgMSAxNCAyMC4yNUg3QTEuMjUgMS4yNSAwIDAgMSA1Ljc1IDE5VjlBMS4yNSAxLjI1IDAgMCAxIDcgNy43NWgxLjI1VjE1QTIuNzUgMi43NSAwIDAgMCAxMSAxNy43NWg0LjI1VjE5Wk0xNyAxNi4yNWgtNkExLjI1IDEuMjUgMCAwIDEgOS43NSAxNVY1QTEuMjUgMS4yNSAwIDAgMSAxMSAzLjc1aDEuNzVWOC41YS43Ni43NiAwIDAgMCAuNzUuNzVoNC43NVYxNUExLjI1IDEuMjUgMCAwIDEgMTcgMTYuMjVaIiAvPjwvc3ZnPg==){.w-4}

Out\[8\]:

``` {.overflow-auto .pb-25 .pr-25}
[{'generated_text': 'What is the moon made of?\n\nWhat is the moon made of?\n\nThe moon is a powerful weapon, but the best example may be on'}]
```

We do get some interesting results, although it is clear that *closed-book* abstractive QA is a challenging task. Larger models store more internal knowledge; thus, closed-book performance is very much tied to model size. With bigger models, we can get better results, but for consistent answers, the open-book alternatives tend to outperform the closed-book approach.

That's it for our article on open-domain question answering (ODQA). We've worked through the idea behind semantic similarity and how it is applied to QA models. We explored the various components that produce these 'QA stacks', like [vector databases](/learn/vector-database/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}, retrievers, readers, and generators. Alongside that, we've learned how to implement these different stacks using different tools and models. All of this should provide a strong foundation for exploring the world and opportunities of ODQA further.

## Further Reading {#Further-Reading .mt-50 .text-h2-mobile .lg:text-h2 .text-black}

-   L. Weng, [How to Build an Open-Domain Question Answering System?](https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}, GitHub Blog
-   D. Khashabi, et al., [UnifiedQA: Crossing Format Boundaries with a Single QA System](https://arxiv.org/pdf/2005.00700.pdf){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline} (2020), EMNLP
-   [Extractive Question Answering](https://huggingface.co/transformers/usage.html#extractive-question-answering){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}, Hugging Face Docs

Share:

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTIiIHZpZXdib3g9IjAgMCAxNCAxMiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJNMTAuNjM2NyAwLjU2MjVIMTIuNTUwOEw4LjMzOTg0IDUuNDAyMzRMMTMuMzE2NCAxMS45Mzc1SDkuNDMzNTlMNi4zNzEwOSA3Ljk3MjY2TDIuODk4NDQgMTEuOTM3NUgwLjk1NzAzMUw1LjQ2ODc1IDYuNzk2ODhMMC43MTA5MzggMC41NjI1SDQuNzAzMTJMNy40Mzc1IDQuMTk5MjJMMTAuNjM2NyAwLjU2MjVaTTkuOTUzMTIgMTAuNzg5MUgxMS4wMTk1TDQuMTI4OTEgMS42NTYyNUgyLjk4MDQ3TDkuOTUzMTIgMTAuNzg5MVoiIGZpbGw9IiM3MTcxN0EiIC8+PC9zdmc+)](https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/series/nlp/question-answering){.group .w-8 .h-8 .flex-col .justify-center .items-center .gap-2.5 .inline-flex .bg-zinc-100 .hover:bg-zinc-200 .cursor-pointer .transition-colors .duration-200 target="_blank" aria-label="Share to Twitter"}[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTMiIGhlaWdodD0iMTMiIHZpZXdib3g9IjAgMCAxMyAxMyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJNMTEuODc1IDAuMTI1QzEyLjMzOTggMC4xMjUgMTIuNzUgMC41MzUxNTYgMTIuNzUgMS4wMjczNFYxMS41QzEyLjc1IDExLjk5MjIgMTIuMzM5OCAxMi4zNzUgMTEuODc1IDEyLjM3NUgxLjM0NzY2QzAuODgyODEyIDEyLjM3NSAwLjUgMTEuOTkyMiAwLjUgMTEuNVYxLjAyNzM0QzAuNSAwLjUzNTE1NiAwLjg4MjgxMiAwLjEyNSAxLjM0NzY2IDAuMTI1SDExLjg3NVpNNC4xOTE0MSAxMC42MjVWNC44MDA3OEgyLjM4NjcyVjEwLjYyNUg0LjE5MTQxWk0zLjI4OTA2IDMuOTgwNDdDMy44NjMyOCAzLjk4MDQ3IDQuMzI4MTIgMy41MTU2MiA0LjMyODEyIDIuOTQxNDFDNC4zMjgxMiAyLjM2NzE5IDMuODYzMjggMS44NzUgMy4yODkwNiAxLjg3NUMyLjY4NzUgMS44NzUgMi4yMjI2NiAyLjM2NzE5IDIuMjIyNjYgMi45NDE0MUMyLjIyMjY2IDMuNTE1NjIgMi42ODc1IDMuOTgwNDcgMy4yODkwNiAzLjk4MDQ3Wk0xMSAxMC42MjVWNy40MjU3OEMxMSA1Ljg2NzE5IDEwLjY0NDUgNC42MzY3MiA4LjgxMjUgNC42MzY3MkM3LjkzNzUgNC42MzY3MiA3LjMzNTk0IDUuMTI4OTEgNy4wODk4NCA1LjU5Mzc1SDcuMDYyNVY0LjgwMDc4SDUuMzM5ODRWMTAuNjI1SDcuMTQ0NTNWNy43NTM5MUM3LjE0NDUzIDYuOTg4MjggNy4yODEyNSA2LjI1IDguMjM4MjggNi4yNUM5LjE2Nzk3IDYuMjUgOS4xNjc5NyA3LjEyNSA5LjE2Nzk3IDcuNzgxMjVWMTAuNjI1SDExWiIgZmlsbD0iIzcxNzE3QSIgLz48L3N2Zz4=)](https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/series/nlp/question-answering){.group .w-8 .h-8 .flex-col .justify-center .items-center .gap-2.5 .inline-flex .bg-zinc-100 .hover:bg-zinc-200 .cursor-pointer .transition-colors .duration-200 target="_blank" aria-label="Share to LinkedIn"} [![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTMiIGhlaWdodD0iMTMiIHZpZXdib3g9IjAgMCAxMyAxMyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJNMTIuNzUgMC4xMjVWMTIuMzc1SDAuNVYwLjEyNUgxMi43NVpNNi45NTMxMiA3LjEyNUw5LjA1ODU5IDMuMTMyODFIOC4xNTYyNUw2LjkyNTc4IDUuNjIxMDlDNi43ODkwNiA1Ljg5NDUzIDYuNjc5NjkgNi4xNDA2MiA2LjU3MDMxIDYuMzU5MzhMNi4yNDIxOSA1LjYyMTA5TDQuOTg0MzggMy4xMzI4MUg0LjAyNzM0TDYuMTMyODEgNy4wNzAzMVY5LjY2Nzk3SDYuOTUzMTJWNy4xMjVaIiBmaWxsPSIjNzE3MTdBIiAvPjwvc3ZnPg==)](https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/series/nlp/question-answering){.group .w-8 .h-8 .flex-col .justify-center .items-center .gap-2.5 .inline-flex .bg-zinc-100 .hover:bg-zinc-200 .cursor-pointer .transition-colors .duration-200 target="_blank" aria-label="Share to Hacker News"}

[PreviousUnsupervised Training for Sentence Transformers](/learn/series/nlp/unsupervised-training-sentence-transformers/){.min-h-[203px] .w-full .max-w-[21.875rem] .rounded-5 .bg-alpha3 .p-25 .transition-colors .duration-200 .hover:bg-alpha4}[NextRetrievers for Question-Answering](/learn/series/nlp/retriever-models/){.min-h-[203px] .w-full .max-w-[21.875rem] .rounded-5 .bg-alpha3 .p-25 .transition-colors .duration-200 .hover:bg-alpha4}

![Learn how to build semantic search systems. From machine transition to question-answering. (series cover image)](/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=3840&q=100){.w-full .max-w-[12.625rem] fetchpriority="high" width="248" height="320" decoding="async" nimg="1" style="color:transparent" sizes="202px" srcset="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=16&q=100 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=32&q=100 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=48&q=100 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=64&q=100 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=96&q=100 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=128&q=100 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=256&q=100 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=384&q=100 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=640&q=100 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=750&q=100 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=828&q=100 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=1080&q=100 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=1200&q=100 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=1920&q=100 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=2048&q=100 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5d5b6937ce94652fbd12baba1dc99bf180cfe589-850x1096.png&w=3840&q=100 3840w"}[Natural Language Processing for Semantic Search](/learn/series/nlp/){.mt-3 .block .text-body .font-semibold .text-alpha2 .hover:underline}

Chapters

1.  [Dense Vectors](/learn/series/nlp/dense-vector-embeddings-nlp/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
2.  [Sentence Transformers and Embeddings](/learn/series/nlp/sentence-embeddings/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
3.  [Training Sentence Transformers with Softmax Loss](/learn/series/nlp/train-sentence-transformers-softmax/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
4.  [Training Sentence Transformers with Multiple Negatives Ranking Loss](/learn/series/nlp/fine-tune-sentence-transformers-mnr/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
5.  [Multilingual Sentence Transformers](/learn/series/nlp/multilingual-transformers/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
6.  [Unsupervised Training for Sentence Transformers](/learn/series/nlp/unsupervised-training-sentence-transformers/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
7.  [An Introduction to Open Domain Question-Answering](/learn/series/nlp/question-answering/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .underline .text-black}
    -   [Question Answering at a Glance](#Question-Answering-at-a-Glance){.hover:underline .hover:opacity-50 .cursor-pointer}
    -   [Further Reading](#Further-Reading){.hover:underline .hover:opacity-50 .cursor-pointer}
8.  [Retrievers for Question-Answering](/learn/series/nlp/retriever-models/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
9.  [Readers for Question-Answering](/learn/series/nlp/reader-models/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
10. [Data Augmentation with BERT](/learn/series/nlp/data-augmentation/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
11. [Domain Transfer with BERT](/learn/series/nlp/domain-transfer/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
12. [Unsupervised Training with Query Generation (GenQ)](/learn/series/nlp/genq/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}
13. [Generative Pseudo-Labeling (GPL)](/learn/series/nlp/gpl/){.transition-colors .duration-200 .hover:opacity-50 .underline-offset-4 .cursor-pointer .text-black .underline}

![](data:image/svg+xml;base64,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){.w-full .-translate-y-[4px]}

Product

[Overview](/product/)[Documentation](https://docs.pinecone.io/)[Integrations](/integrations/)[Trust and Security](/security/)

Solutions

[Customers](/customers/)[RAG](/solutions/rag/)[Semantic Search](/solutions/semantic/)[Multi-Modal Search](/solutions/multi-modal/)[Candidate Generation](/solutions/candidate-generation/)[Classification](/solutions/classification/)

Resources

[Learning Center](/learn/)[Community](/community/)[Pinecone Blog](/blog/)[Support Center](https://support.pinecone.io/)[System Status](https://status.pinecone.io/)[What is a Vector Database?](/learn/vector-database/)[What is Retrieval Augmented Generation (RAG)?](/learn/retrieval-augmented-generation/)

Company

[About](/company/)[Partners](/partners/)[Careers](/careers/)[Newsroom](/newsroom/)[Contact](/contact/)

Legal

[Customer Terms](/legal/)[Website Terms](/terms/)[Privacy](/privacy/)[Cookies](/cookies/)[Cookie Preferences](#)

© Pinecone Systems, Inc. \| San Francisco, CA

Pinecone is a registered trademark of Pinecone Systems, Inc.
